An introduction to eventual consistency and what it means in terms of handling data with Riak.

<div id="toc"></div>

h2. A simple example of eventual consistency

This basis for a simple example is a Riak cluster with five nodes and a default quorum of 3. That means every piece of data exists
three times in this cluster. In this setup reads use a quorum of 2 to ensure at least two copies, whereas writes also use a quorum
of 2 to enforce full consistency. Remember that R + W > N ensures full consistency in a cluster.

When data is written with a quorum of 2, Riak sends the write request to all three replicas anyway, but returns a successful reply
when two of them acknowledged a successful write on their end. One node may even have been down, not acknowledging the write.

The purpose of this list of examples is to describe how the replica node that just failed gets its data back, so that at some
undetermined point in the future, all replicas of a piece of data will be up-to-date. There are ways of controlling when a specific
piece of data is consistent across all its replicas, but either way, the data will eventually be consistent on all of them.

h2. Anatomy of a Riak Request

To understand how eventual consistency is handled in a Riak environment, it's important to know how a request is handled internally.
There's not much magic involved, but helps understand things like read repair, and how the quorum is handled in read and write
requests. Be sure to read the wiki page on [[Replication#Understanding-replication-by-example]] first, it has all the details on how
data is replicated across nodes and what happens when a node becomes unavailable for read and write requests, the basics for how
eventual consistency is handled in Riak.

Recall that every key belongs to N primary virtual nodes and a number of secondary nodes. Primary nodes are the ones that are physically responsible for vnode at a
given time. Secondaries are fallback nodes, they're logically close to the primaries in the key space, and they're the nodes to go to should a primary become
unavailable.

The basic steps of a request in Riak are the following.

* Determine the nodes responsible for the key from the preference list
* Send a request to all the nodes determined in the previous step
* Wait until enough requests returned the data to fulfill the read quorum (if specified) or the basic quorum
* Return the value to the client

The steps are similar for both read and write requests, with some details different, we'll go into the differences in the examples
below.

In our example cluster, we'll assume that it's healthy and all nodes are available, that means sending requests to three replicas of
the key requested.

h2. Failure Scenarios

Now we'll go through a bunch of failure scenarios that could result in data inconsistencies, and we'll explore how they're resolved. Every scenario assumes a
healthy cluster to begin with, where all nodes are available.

In a typical failure scenario, at least one node goes down, leaving two replicas intact in the cluster. Clients can expect that reads with an R of 2 will still
succeed, until the third replica comes back up again. It's up to the application's details to implement some sort of graceful degradation in an automated fashion
or as a feature flip that can be tuned at runtime accordingly, or to simply retry when a piece of data is expected to be found, but a first request returns a
not_found.

h3. One Primary Fails

* Data is written to a key with W=3
* One node goes down, it happens to be a primary for that key
* Data is read from that key with R=3
* Riak returns not_found on first request
* Read repair ensures data is replicated to a secondary node
* Subsequent reads return correct value with R=3

h3. Two Primaries Fail

* Data is written to a key with W=3
* Two nodes go down, they happen to be primaries for that key
* Data is read from that key with R=3
* Riak returns not_found on first request
* Read repair ensures data is replicated to a secondary node

Similar scenario to the above, but initial read consistency expectations may degrade even further, leaving only one initial replica.

h3. Edge Case: Two Primaries Fails, R=1

When two primaries go down, leaving only one replica, and clients read using an R value of 1, something that may be unexpected and even confusing happens. The
current implementation of get in Riak involves a mechanism called basic quorum. The basic quorum assumes that if a majority of nodes return that an object wasn't
found, then the node coordinating the request assumes the object doesn't exist, even if one node would have the data available.

Just like above, subsequent requests would yield the expected results, as read repair ensured that the data now resides on two secondary nodes.

<div class="note"><div class="title">Basic Quorum</div>Basic quorum requires the majority of nodes to return a not_found for a request to be successful. The simple minority is enough, and is calculated using @truncate(N / 2.0 + 1)@, so for an N value of 3 at least 2 nodes must return a value for a successful request

The potential for confusion around the basic quorum has been addressed in the "current development of Riak":https://issues.basho.com/show_bug.cgi?id=992, and the next major release will include an option to disable the basic quorum for a specific request.</div>

h3. Three Primaries Fail

* Data is written to a key with W=3
* All primary nodes responsible for the key go down
* Data is read using R=3 (or any quorum)

This incident will always yield a not found error, as no node is able to serve the request.

h2. Further Reading

* Werner Vogels, et. al.: "Eventually Consistent - Revisited":http://www.allthingsdistributed.com/2008/12/eventually_consistent.html
* Ryan Zezeski: "Riak Core, First Multinode":https://github.com/rzezeski/try-try-try/tree/master/2011/riak-core-first-multinode,
"Riak Core, The vnode":https://github.com/rzezeski/try-try-try/tree/master/2011/riak-core-the-vnode, "Riak Core, The
Coordinator":https://github.com/rzezeski/try-try-try/tree/master/2011/riak-core-the-coordinator
